## 📌 1. 퍼셉트론(Perceptron)
퍼셉트론은 인공지능의 가장 기초가 되는 신경망 모델
뇌의 뉴런(Neuron)처럼 동작하며, 여러 입력을 받아서 출력을 0 또는 1로 결정하는 역할

예를 들어, 퍼셉트론이 AND 연산을 학습한다고 가정:
✅ (0, 0) → 0
✅ (0, 1) → 0
✅ (1, 0) → 0
✅ (1, 1) → 1
이런 방식으로 출력을 결정.

## 📌 2. 퍼셉트론의 수학적 원리
퍼셉트론은 다음 수식을 기반으로 동작.

𝑦 = 𝑓(𝑊𝑋 + 𝑏)

- `𝑋` = 입력 데이터 (예: [𝑥1,𝑥 2])
- `𝑊` = 가중치 (각 입력의 중요도를 결정)
- `𝑏` = 편향(bias, 기본값 조정)
- `𝑓` = 활성화 함수 (출력을 0 또는 1로 변환)

### ✅ 1단계: 가중치와 입력의 곱 계산
퍼셉트론은 각 입력 값에 가중치를 곱해서 더함.

𝑆 = 𝑊_1𝑥_1 + 𝑊_2𝑥_2 + 𝑏
계산된 S 값이 일정 기준(Threshold) 이상이면 1,
아니면 0을 출력하도록 활성화 함수를 적용.

## 📌 3. 활성화 함수 (Activation Function)
퍼셉트론에서 중요한 건 출력을 0 또는 1로 바꾸는 활성화 함수
가장 단순한 활성화 함수 : **계단 함수(Step Function)**.

𝑓(𝑆) = 1 (if 𝑆≥0), 0 (if 𝑆<0)
 
즉, 가중치와 입력의 곱을 계산한 값이 0 이상이면 1,
그렇지 않으면 0을 출력.

## 📌 4. 퍼셉트론 학습 과정 (Gradient Descent)
퍼셉트론이 제대로 동작하려면 **가중치 `𝑊`와 편향 `𝑏`**를 조정해야 함.
이 과정이 학습(Training)

### 1️⃣ 예측값 계산
𝑦_𝑝𝑟𝑒𝑑 = 𝑓(𝑊𝑋+𝑏)

### 2️⃣ 오차(error) 계산
퍼셉트론의 예측값이 정답과 다를 수 있음.
이때, 오차(error)를 계산.

error = 𝑦_𝑡𝑟𝑢𝑒 − 𝑦_𝑝𝑟𝑒𝑑

### 3️⃣ 가중치 업데이트
가중치를 조정하는 공식

𝑊=𝑊+𝛼×error×𝑋
- `𝛼` : 학습률 (Learning Rate) ; 조정하는 속도

## 📌 5. 퍼셉트론 학습 예제
예를 들어, AND 연산을 학습한다고 해보자.

x1	x2	y(정답)
0	0	0
0	1	0
1	0	0
1	1	1
초기 가중치: 𝑊_1=0, 𝑊_2=0, 𝑏=0

1️⃣ (0,0) 입력 → 𝑦_𝑝𝑟𝑒𝑑 = 0 → 정답과 일치 → 가중치 변화 없음
2️⃣ (0,1) 입력 → 𝑦_𝑝𝑟𝑒𝑑 = 0 ​→ 정답과 일치 → 가중치 변화 없음
3️⃣ (1,0) 입력 → 𝑦_𝑝𝑟𝑒𝑑 = 0 → 정답과 일치 → 가중치 변화 없음
4️⃣ (1,1) 입력 → 𝑦𝑝𝑟𝑒𝑑 = 0 → 정답(1)과 다름 → 가중치 업데이트

𝑊1=𝑊1+𝛼(1−0)×1
이런 방식으로 학습이 반복되면서 가중치가 최적의 값으로 조정
결과적으로, 퍼셉트론이 AND 연산을 학습할 수 있음

📌 6. 퍼셉트론이 풀 수 없는 문제
퍼셉트론은 단순한 논리 문제(AND, OR)는 풀 수 있지만,
XOR(배타적 OR) 문제는 풀 수 없음.
XOR은 직선으로 나눌 수 없기 때문

✅ 해결 방법? → 다층 퍼셉트론(MLP)을 사용
즉, 뉴런을 여러 개 쌓아서 비선형 문제를 해결.
